---
title: "TOMTOM Cutoff Investigation"
author: "Matt Richards"
date: "6/16/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Description

We ran TOMTOM for all JASPAR 2016 motifs against themselves. There are 1209 motifs, so this should theoretically give us 1209 absolute matches; however, this will not actually be the case. We are hoping to establish a p-value threshold, above which the matches are insignificant and below which they are significant. 

First, we'll load the data and give it the proper column names:

```{r}
jaspar.data <- read.table(file = "./tomtom.txt")
names(jaspar.data) <- c("Query.ID", "Target.ID", "Optimal.offset", "Pvalue","Evalue","Qvalue","Overlap","Query.consensus","Target.consensus","Orientation")
```

Just to get an idea of the data distribution, here's some summary stats:

```{r}
dim(jaspar.data)
summary(jaspar.data$Pvalue)
```

## Choosing a cutoff

Let's expand upon the summary and graphically represent our P-value distribution. We'll also trim the data to leave only a few columns

```{r message = FALSE}
library(dplyr); library(ggplot2)

jaspar.data <- select(jaspar.data, 1:4)

ggplot(jaspar.data, aes(x=Pvalue)) + geom_histogram()

```

There's about 50,000 things in the smallest bucket, and that's many more than our 1209 motifs; let's filter using 10^-10 as our cutoff and try again:

```{r message = FALSE}

first.cutoff <- filter(jaspar.data, Pvalue < 1e-10)
ggplot(first.cutoff, aes(x=Pvalue)) + geom_histogram()
```

Getting close now; that's the correct magnitude of data, but still too many records. Let's go for 5 more orders of magnitude

```{r message = FALSE}

second.cutoff <- filter(jaspar.data, Pvalue < 1e-15)
ggplot(second.cutoff, aes(x=Pvalue)) + geom_histogram()
```

We're getting there now; one more jump should do it:

```{r message = FALSE}

third.cutoff <- filter(jaspar.data, Pvalue < 1e-20)
ggplot(third.cutoff, aes(x=Pvalue)) + geom_histogram()
```
```{r}

dim(second.cutoff)
dim(third.cutoff)
```

Let's assume the third cutoff is our chosen point; how many unique motifs are then determined to be duplicates (assuming the queries are the duplicates, not the targets)

```{r}
length(unique(second.cutoff$Query.ID))
length(unique(third.cutoff$Query.ID))
```

This is rather like taking stabs in the dark; we should illustrate using sensitivity/specificity and a ROC curve. Let's make a function:

```{r}

# There are 1208*1209 possible negatives (1208 for each); there are only 1209 positives

createROCDF <- function(jaspar.data){
  
  # Create a space of P-values
  p <-sapply(seq(1,20, 0.1), function(x) 10^(-x))
  logp <- seq(1,20,0.1)
  sensitivity <- numeric(length = length(p))
  specificity <- numeric(length = length(p))
  positives <- numeric(length = length(p))
  true.positives <- numeric(length = length(p))
  
  # Filter for each one
  for (i in 1:length(p)){
    my.data <- filter(jaspar.data, Pvalue < p[i])
    
    sensitivity[i] <- length(unique(my.data$Query.ID))/length(my.data$Query.ID)
    specificity[i] <- (1209*1208-sum(duplicated(my.data$Query.ID)))/(1209^2 - length(my.data$Query.ID))
    positives[i] <- length(my.data$Query.ID)
    true.positives[i] <- length(unique(my.data$Query.ID))
  }
  
  roc.df <- data.frame(p=p, 
                       logp = logp, 
                       sensitivity=sensitivity, 
                       specificity=specificity, 
                       positives = positives, 
                       true.positives = true.positives)
  
}
```

Now we'll make the data frame and plot the ROC curve, which is sensitivity v. 1-specificity:

```{r}
roc.df <- createROCDF(jaspar.data = jaspar.data)

plot(1 - roc.df$specificity, roc.df$sensitivity) 
```

It looks like a typical ROC curve, but the x-axis is tiny; this is because our false positive rate (1-specificity) is really small. Essentially, there's around 1.5 million known negatives in the dataset, and FPR is FP/N. Anything that doesn't show up in our dataset is considered "negative" automatically, thus our denominator will always be around 1.4 million whereas our numerator theoretically tops out at ~100,000. We're pinned to a specificity of 90%+, even if we don't filter out any P-values, thus our FPR (1- specificity) is always going to be under 10%, and generally much lower than that. 

What we CAN do is study our sensitivity, which ranges from 0 to ~ 1. Here's some summary stats on the data frame in general, including the sensitivity:

```{r} 
summary(roc.df)
```

Based upon this summary, we're probably not going to do any better than ~ 90% sensitivity. Let's take a look at the P-values in that region:

```{r}
high.sens <- roc.df %>% filter(sensitivity >= 0.9) %>% select(logp, sensitivity, positives, true.positives)
summary(high.sens$logp)
```

Basically, once we get above a certain value of -log(p), we're screening so rigorously that our positives are pretty reliably true. However, we might be missing a lot of our known positive (matching) cases. Let's take a look at all our columns at the top and bottom of the data set:

```{r}
head(high.sens)
tail(high.sens)
```

Comparing the top and bottom of the data frame, we have identified 88 fewer "positives", but in doing so we've dropped 54 "true positives". We've increased our sensitivity because {r 54/88} is much lower than 90%, our sensitivity threshold. What is clear is that we're in a pretty stringent portion of our ROC curve; we could stand to be a lot more liberal, so let's ask the following question: what is the minimum P-value we can set as a threhold that captures all 1209 unique true positives?

We'll go back to our full data frame and pull out only the ones that grab ALL true positives:

```{r}
all.tp <- filter(roc.df, true.positives == 1209)
dim(all.tp)
summary(all.tp)
```

As shown, we're left with 64 values; our minimum p-value cutoff is now 5 x 10^{-8}; however, that clearly comes with its own problems, as our sensitivity there is only 28%. Let's see how much different we can be when capturing 95% of the true motifs (1149). 

```{r}

most.tp <- filter(roc.df, true.positives >= 1149)
dim(most.tp)
summary(most.tp)
```

The log-Pvalues capture the fact that we capture 1152 at p = 10^-14.7; in fact, we know that if we lower log(p) to -15, we will get 1144 true positives and 1387 total. Perhaps most importantly, we're now back in a more reasonable sensitivity area of the curve. So let's summarize what we know:

## Investigation using P-value plotting

We can also look at this problem from the other angle; that is, if we take the top hit or two for each query, what does the distribution of p-values look like?  First, let's assemble our dataset, starting with the full jaspar dataset again. We'll grab all the hits of each query with itself, which should be the top hit for each case:

```{r}

firsts <- jaspar.data %>% filter(Query.ID == Target.ID)
dim(firsts)
```

As expected, there's 1209 hits here. We'll log the P-values and then make a plot of them:

```{r}
firsts %>% mutate(logP = -1*log10(Pvalue)) %>%
  ggplot(aes(x=logP)) + geom_histogram()
```

As we might expect, the threshold of ~15 is right in the interesting region where the histogram seems to show a marked increase. So let's summarize what we've found:

1. Specificity is a non-issue; we'll be pretty specific in any case
2. High sensitivity can be achieved, but at the expense of true positives
3. Perfect true positive accuracy requires sensitivity of < 28%, which is likely not what we want
4. A -log(P) of ~15 gives us a pretty good compromise between sensitivity and true positive capture rate. 

With that in mind, we'll move on to seeing what the implications are for HOCOMOCO.

## HOCOMOCO Redundancy Analysis

The HOCOMOCO/Jaspar comparisons are found in 2 separate files that we'll read in now:

```{r}

human.hoco <- read.table(file = "../hocomoco_jaspar_human/tomtom.txt")
mouse.hoco <- read.table(file = "../hocomoco_jaspar_mouse/tomtom.txt")
all.hoco <- rbind(human.hoco,mouse.hoco)
names(all.hoco) <- c("Query.ID", "Target.ID", "Optimal.offset", "Pvalue","Evalue","Qvalue","Overlap","Query.consensus","Target.consensus","Orientation")
dim(all.hoco)
```

We've got 30,398 records here; in each case, HOCOMOCO provides the query and Jaspar provides the target, as demonstrated:

```{r}
head(all.hoco$Query.ID)
```

As evidenced by the output, there's 957 different motifs from HOCOMOCO. Just out of curiosity, let's look at how their P-values are distributed.

```{r}
all.hoco <- all.hoco %>% mutate(logP = -1*log10(Pvalue))
  ggplot(all.hoco, aes(x=logP)) + geom_histogram()
```

We've got almost nothing above the threshold of 15; even above 8 there's not much. What's the actual dimensions of those datasets?

```{r}
dim(all.hoco %>% filter(logP >= 15))
dim(all.hoco %>% filter(logP >= 8))
```

Let's go ahead and take our value of 5 x 10^-8 from before, corresponding to a logP of 7.3; that's the threshold that cuts off finding ALL of our JASPAR motifs. Using that value, we'll narrow down our dataset. We'll also create a summary of how many times each motif is there

```{r}
above.p <- all.hoco %>% filter(logP >= 7.3)

above.p %>% 
  group_by(Query.ID) %>% 
  summarise(Num.Hits = n(), Max.p = max(Pvalue), Min.p = min(Pvalue)) %>% 
  arrange(desc(Num.Hits))
```

Our top hit-getter is "Hsapiens-HOCOMOCOv10-GSX2_HUMAN.H10MO.D" and we have 417 unique values here, with ~170 having only 1 match and ~100 having only 2 matches. As a sanity check, let's check out the top hit-getter and see what its target IDs and sequences look like:

```{r}
above.p %>% filter(Query.ID == "Hsapiens-HOCOMOCOv10-GSX2_HUMAN.H10MO.D") %>% 
  arrange(Pvalue) %>% select(Target.ID, Target.consensus, Query.consensus, Pvalue) 

```

This looks pretty good; let's proceed forward and narrow down our HOCOMOCO/Jaspar motif list to take away these HOCOMOCO motifs that have matches below our threshold. 
## Producing the Final FIMO Motif List

We would like a list of the motifs we want in FIMO so that we can use them to create our database. Let's grab this list of unique HOCOMOCO IDs and then use MotifDb to screen them out. 

```{r}
repeat.motifs <- unique(above.p$Query.ID)
length(repeat.motifs)
```

As expected, there's 417 motifs here that we want to remove. Now then, let's load up MotifDb and grab the list of all human and mouse motifs in HOCOMOCO and Jaspar; we'll start by isolating based on data source:

```{r message=FALSE}

library(MotifDb)
all.motifs <- subset(MotifDb, dataSource %in% c("HOCOMOCOv10","jaspar2016"))
length(all.motifs)
```

As we see, there's 2275 motifs; 1209 from Jaspar, 1066 from HOCOMOCO. This is a good sanity check to make sure we're on track; now we'll narrow it down by species:

```{r}
all.motifs <- subset(all.motifs, organism %in% c("Hsapiens","Mmusculus"))
length(all.motifs)
```

We shed about 600 motifs, which makes sense as HOCOMOCO is entirely made up of these 2 species, so we're losing about half of Jaspar. As a further sanity check, we'll look at the head and tail of our list:

```{r}
head(all.motifs)
tail(all.motifs)
```

Looks exactly like what we'd expect; we're starting with a list of 1673 motifs and now it's time to remove our redundant ones:

```{r}
fimo.motifs <- subset(all.motifs, !(providerId %in% repeat.motifs))
length(fimo.motifs)
```

This doesn't work because the providerId field chops off the "Hsapiens/Mmusculus-HOCOMOCOv10-" bit; so let's modify our list of matching strings so we actually get the matches

```{r}
trimmed.repeats <- gsub(".+HOCOMOCOv10-","",repeat.motifs)
head(trimmed.repeats)
```

Looks good; we'll try matching on these now:

```{r}
fimo.motifs <- subset(all.motifs, !(providerId %in% trimmed.repeats))
length(fimo.motifs)
```

This new set is exactly 417 motifs shorter than before; it's exactly what we want. So we'll finish our analysis by saving this as a .meme file that we can use for creating our new FIMO database:

```{r}
export(fimo.motifs, "./final_fimo_motifs.meme",format="meme")
```

## Expanding the Motifs

In comparing with our previous table, we found that our human and mouse motifs from Jaspar didn't cover everything we had before. Instead, several other vertebrates popped up, forcing us to consider these as well. 

```{r}
other.vertebrates <- c("Ggallus","Ocuniculus","Rnorvegicus","Rrattus","Xlaevis")
```

